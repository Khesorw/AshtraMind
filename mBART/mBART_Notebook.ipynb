{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e718041c",
   "metadata": {},
   "source": [
    "# 1_Setup_and_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f056607",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports are successful ✅\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0]\n",
      "Pip version: 25.1\n",
      "Pytorch version: 2.7.0+cu126\n",
      "CUDA version: 12.6\n",
      "GPU is available.\n",
      "Number of GPUs: 2\n",
      "GPU 0: NVIDIA GeForce RTX 4070\n",
      "GPU 1: NVIDIA GeForce RTX 4070\n",
      "Pytorch can use CUDA ✅Tensor on GPU\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'exact_match': np.float64(1.0)}\n",
      "Evaluate is working ✅\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import sys\n",
    "import pip\n",
    "import torch\n",
    "from datasets import get_dataset_split_names, load_dataset, load_dataset_builder, get_dataset_config_names,  load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, MBart50Tokenizer, MBartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "print(\"All imports are successful ✅\")\n",
    "\n",
    "print(\"--\" * 50)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Check Python, pip, and pytorch versions and cuda compatibility\n",
    "#---------------------------------------------------------------\n",
    "print(\"Python version:\", sys.version)\n",
    "# Print pip version\n",
    "print(\"Pip version:\", pip.__version__)\n",
    "# Print pytorch version\n",
    "print(\"Pytorch version:\", torch.__version__)\n",
    "# Print CUDA version\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Print GPU information\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Check if pytorch can use CUDA\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.rand(5, 3).cuda()\n",
    "    if x.is_cuda:\n",
    "        print(\"Pytorch can use CUDA ✅Tensor on GPU\")\n",
    "else:\n",
    "    print(\"Pytorch is not using CUDA.\")\n",
    "\n",
    "print(\"--\" * 50)\n",
    "# Check if evaluate is working\n",
    "!python -c \"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))\"\n",
    "print(\"Evaluate is working ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30206b71",
   "metadata": {},
   "source": [
    "# 2_Load_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80dbfe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset splits: ['train', 'validation', 'test']\n",
      "Available dataset configurations: ['Itihasa']\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/load_hub\n",
    "splits = get_dataset_split_names(\"rahular/itihasa\")\n",
    "print(\"Available dataset splits:\", splits)\n",
    "configs = get_dataset_config_names(\"rahular/itihasa\")\n",
    "print(\"Available dataset configurations:\", configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec7db925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': Translation(languages=['sn', 'en'], id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder = load_dataset_builder(\"rahular/itihasa\")\n",
    "\n",
    "# Inspect dataset description\n",
    "ds_builder.info.description\n",
    "\n",
    "# Inspect dataset features\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dafb5cb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully ✅.\n",
      "Train dataset size: 75162\n",
      "Validation dataset size: 6149\n",
      "Test dataset size: 11722\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"rahular/itihasa\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"rahular/itihasa\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"rahular/itihasa\", split=\"test\")\n",
    "print(\"Datasets loaded successfully ✅.\")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb1a17ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'The ascetic Vālmīki asked Nārada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.',\n",
       "  'sn': 'ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुङ्गवम्॥'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]  # Inspect the first example in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "195509c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Hearing the words of Viśvāmitra, Rāghava, together with Laksmana, was struck with amazement, and spoke to Viśvāmitra, saying,',\n",
       "  'sn': 'विश्वामित्रवचः श्रुत्वा राघवः सहलक्ष्मणः। विस्मयं परमं गत्वा विश्वामित्रमथाब्रवीत्॥'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]  # Inspect the first example in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c896d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'When Şītā, having a husband although seeming as if she had none, was putting on the ascetic guise, the people got into a wrath and exclaimed, “O Dasaratha, fie on you!\"',\n",
       "  'sn': 'तस्यां चीरं वसानायां नाथवत्यामनाथवत्। प्रचुक्रोश जनः सर्वो धिक् त्वां दशरथं त्विति ॥'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0] # Inspect the first example in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e8a56f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'The ascetic Vālmīki asked Nārada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.', 'sn': 'ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुङ्गवम्॥'}}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'en': 'The ascetic Vālmīki asked Nārada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.', 'sn': 'ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुङ्गवम्॥'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The ascetic Vālmīki asked Nārada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुङ्गवम्॥\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example 0: (English: The ascetic Vālmīki asked Nārada, the best of sages and foremost of those conversant with words, ever engaged in austerities and Vedic studies.) (Sanskrit: ॐ तपः स्वाध्यायनिरतं तपस्वी वाग्विदां वरम्। नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुङ्गवम्॥)\n",
      "Example 1: (English: Who at present in this world is like crowned with qualities, and with prowess, knowing duty, and grateful, and truthful, and firm in vow.) (Sanskrit: कोन्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्। धर्मज्ञश्च कृतज्ञश्च सत्यवाक्यो दृढत्नतः॥)\n",
      "Example 2: (English: Who is qualified by virtue of his character, and who is engaged in the welfare of all creatures? Who is learned and capable. Who alone is ever lovely to behold?) (Sanskrit: चारित्रेण च को युक्तः सर्वभूतेषु को हितः। विद्वान् कः कः समर्थश्च कश्चैकप्रियदर्शनः॥)\n"
     ]
    }
   ],
   "source": [
    "# Indexing the datasets\n",
    "print(train_dataset[0])  # To see the full content of the first example\n",
    "print(\"--\" * 50)\n",
    "print(train_dataset[0][\"translation\"])  # To see the root of the nested dictionary\n",
    "print(\"--\" * 50)\n",
    "print(train_dataset[0][\"translation\"][\"en\"])  # To see the English translation of the first example\n",
    "print(\"--\" * 50)\n",
    "print(train_dataset[0][\"translation\"][\"sn\"])  # To see the Sanskrit translation of the first example\n",
    "print(\"--\" * 50)\n",
    "for i in range(3):\n",
    "    print(f\"Example {i}: (English: {train_dataset[i]['translation']['en']}) (Sanskrit: {train_dataset[i]['translation']['sn']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5cd3a6",
   "metadata": {},
   "source": [
    "# 3_Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ed4df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "TOKENIZER = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "TOKENIZER.src_lang = \"en_XX\"\n",
    "TOKENIZER.tgt_lang = \"hi_IN\"  # Setting Hindi token id as a proxy for Sanskrit\n",
    "\n",
    "\n",
    "TEXT_TO_TRANSLATE = \"For one who has conquered the mind, the mind is the best of friends; but for one who has failed to do so, his very mind will be the greatest enemy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20f1cbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'जिसने मन पर विजय प्राप्त की है तो वह सबसे अच्छा मित्र है, पर जिसने ऐसा नहीं किया तो उसका मन ही सबसे बड़ा शत्रु होगा।'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_text(text, model=MODEL, tokenizer=TOKENIZER, src_lang=TOKENIZER.src_lang, tgt_lang=TOKENIZER.tgt_lang, skip_special_tokens=True):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Force decoder to use target language\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "translate_text(text=TEXT_TO_TRANSLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba7a363e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total languages supported by the tokenizer: 52\n",
      "Language Code: ar_AR, Human Name: Arabic, Token ID: 250001\n",
      "Language Code: cs_CZ, Human Name: Czech, Token ID: 250002\n",
      "Language Code: de_DE, Human Name: German, Token ID: 250003\n",
      "Language Code: en_XX, Human Name: English, Token ID: 250004\n",
      "Language Code: es_XX, Human Name: Spanish, Token ID: 250005\n",
      "Language Code: et_EE, Human Name: Estonian, Token ID: 250006\n",
      "Language Code: fi_FI, Human Name: Finnish, Token ID: 250007\n",
      "Language Code: fr_XX, Human Name: French, Token ID: 250008\n",
      "Language Code: gu_IN, Human Name: Gujarati, Token ID: 250009\n",
      "Language Code: hi_IN, Human Name: Hindi, Token ID: 250010\n",
      "Language Code: it_IT, Human Name: Italian, Token ID: 250011\n",
      "Language Code: ja_XX, Human Name: Japanese, Token ID: 250012\n",
      "Language Code: kk_KZ, Human Name: Kazakh, Token ID: 250013\n",
      "Language Code: ko_KR, Human Name: Korean, Token ID: 250014\n",
      "Language Code: lt_LT, Human Name: Lithuanian, Token ID: 250015\n",
      "Language Code: lv_LV, Human Name: Latvian, Token ID: 250016\n",
      "Language Code: my_MM, Human Name: Burmese, Token ID: 250017\n",
      "Language Code: ne_NP, Human Name: Nepali, Token ID: 250018\n",
      "Language Code: nl_XX, Human Name: Dutch, Token ID: 250019\n",
      "Language Code: ro_RO, Human Name: Romanian, Token ID: 250020\n",
      "Language Code: ru_RU, Human Name: Russian, Token ID: 250021\n",
      "Language Code: si_LK, Human Name: Sinhala, Token ID: 250022\n",
      "Language Code: tr_TR, Human Name: Turkish, Token ID: 250023\n",
      "Language Code: vi_VN, Human Name: Vietnamese, Token ID: 250024\n",
      "Language Code: zh_CN, Human Name: Chinese (Simplified), Token ID: 250025\n",
      "Language Code: af_ZA, Human Name: Afrikaans, Token ID: 250026\n",
      "Language Code: az_AZ, Human Name: Azerbaijani, Token ID: 250027\n",
      "Language Code: bn_IN, Human Name: Bengali, Token ID: 250028\n",
      "Language Code: fa_IR, Human Name: Persian, Token ID: 250029\n",
      "Language Code: he_IL, Human Name: Hebrew, Token ID: 250030\n",
      "Language Code: hr_HR, Human Name: Croatian, Token ID: 250031\n",
      "Language Code: id_ID, Human Name: Indonesian, Token ID: 250032\n",
      "Language Code: ka_GE, Human Name: Georgian, Token ID: 250033\n",
      "Language Code: km_KH, Human Name: Khmer, Token ID: 250034\n",
      "Language Code: mk_MK, Human Name: Macedonian, Token ID: 250035\n",
      "Language Code: ml_IN, Human Name: Malayalam, Token ID: 250036\n",
      "Language Code: mn_MN, Human Name: Mongolian, Token ID: 250037\n",
      "Language Code: mr_IN, Human Name: Marathi, Token ID: 250038\n",
      "Language Code: pl_PL, Human Name: Polish, Token ID: 250039\n",
      "Language Code: ps_AF, Human Name: Pashto, Token ID: 250040\n",
      "Language Code: pt_XX, Human Name: Portuguese, Token ID: 250041\n",
      "Language Code: sv_SE, Human Name: Unknown, Token ID: 250042\n",
      "Language Code: sw_KE, Human Name: Unknown, Token ID: 250043\n",
      "Language Code: ta_IN, Human Name: Tamil, Token ID: 250044\n",
      "Language Code: te_IN, Human Name: Telugu, Token ID: 250045\n",
      "Language Code: th_TH, Human Name: Thai, Token ID: 250046\n",
      "Language Code: tl_XX, Human Name: Tagalog, Token ID: 250047\n",
      "Language Code: uk_UA, Human Name: Ukrainian, Token ID: 250048\n",
      "Language Code: ur_PK, Human Name: Urdu, Token ID: 250049\n",
      "Language Code: xh_ZA, Human Name: Xhosa, Token ID: 250050\n",
      "Language Code: gl_ES, Human Name: Galician, Token ID: 250051\n",
      "Language Code: sl_SI, Human Name: Slovenian, Token ID: 250052\n"
     ]
    }
   ],
   "source": [
    "# Human-readable language names mapped to mBART-50 language codes\n",
    "lang_code_to_name = {\n",
    "    \"ar_AR\": \"Arabic\", \"cs_CZ\": \"Czech\", \"de_DE\": \"German\", \"en_XX\": \"English\", \"es_XX\": \"Spanish\",\n",
    "    \"et_EE\": \"Estonian\", \"fi_FI\": \"Finnish\", \"fr_XX\": \"French\", \"gu_IN\": \"Gujarati\", \"hi_IN\": \"Hindi\",\n",
    "    \"it_IT\": \"Italian\", \"ja_XX\": \"Japanese\", \"kk_KZ\": \"Kazakh\", \"ko_KR\": \"Korean\", \"lt_LT\": \"Lithuanian\",\n",
    "    \"lv_LV\": \"Latvian\", \"my_MM\": \"Burmese\", \"ne_NP\": \"Nepali\", \"nl_XX\": \"Dutch\", \"ro_RO\": \"Romanian\",\n",
    "    \"ru_RU\": \"Russian\", \"si_LK\": \"Sinhala\", \"tr_TR\": \"Turkish\", \"vi_VN\": \"Vietnamese\", \"zh_CN\": \"Chinese (Simplified)\",\n",
    "    \"af_ZA\": \"Afrikaans\", \"az_AZ\": \"Azerbaijani\", \"bn_IN\": \"Bengali\", \"fa_IR\": \"Persian\", \"he_IL\": \"Hebrew\",\n",
    "    \"hr_HR\": \"Croatian\", \"id_ID\": \"Indonesian\", \"ka_GE\": \"Georgian\", \"km_KH\": \"Khmer\", \"mk_MK\": \"Macedonian\",\n",
    "    \"ml_IN\": \"Malayalam\", \"mn_MN\": \"Mongolian\", \"mr_IN\": \"Marathi\", \"pl_PL\": \"Polish\", \"ps_AF\": \"Pashto\",\n",
    "    \"pt_XX\": \"Portuguese\", \"sr_XX\": \"Serbian\", \"ta_IN\": \"Tamil\", \"te_IN\": \"Telugu\", \"th_TH\": \"Thai\",\n",
    "    \"tl_XX\": \"Tagalog\", \"uk_UA\": \"Ukrainian\", \"ur_PK\": \"Urdu\", \"xh_ZA\": \"Xhosa\", \"gl_ES\": \"Galician\",\n",
    "    \"sl_SI\": \"Slovenian\"\n",
    "}\n",
    "\n",
    "# Print total number of languages\n",
    "print(\"Total languages supported by the tokenizer:\", len(TOKENIZER.lang_code_to_id))\n",
    "\n",
    "# Print human-readable name for each language code\n",
    "for lang_code, token_id in TOKENIZER.lang_code_to_id.items():\n",
    "    name = lang_code_to_name.get(lang_code, \"Unknown\")\n",
    "    print(f\"Language Code: {lang_code}, Human Name: {name}, Token ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355dedee",
   "metadata": {},
   "source": [
    "# 4_Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41eb4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 1068\n",
      "Top 10 longest: [451, 460, 484, 485, 642, 668, 794, 814, 911, 1068]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of input IDs for each Sanskrit translation in the training dataset\n",
    "# This will help to select max length for model inputs in the preprocess function\n",
    "# Extract list of Sanskrit texts\n",
    "# Sanskrit contains lot of samasa (compound words) which can be long therefore appropriate to check token lengths\n",
    "sanskrit_texts = [item[\"translation\"][\"sn\"] for item in train_dataset]\n",
    "\n",
    "# Now calculate token lengths\n",
    "token_lens = [len(TOKENIZER(text)[\"input_ids\"]) for text in sanskrit_texts]\n",
    "\n",
    "# Check maximum and top 10 longest\n",
    "print(\"Max length:\", max(token_lens))\n",
    "print(\"Top 10 longest:\", sorted(token_lens)[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fc90eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [t[\"en\"] for t in examples[\"translation\"]]\n",
    "    targets = [t[\"sn\"] for t in examples[\"translation\"]]  # Sanskrit texts\n",
    "    \n",
    "    model_inputs = TOKENIZER(inputs, max_length=512, truncation=True, padding=False)\n",
    "\n",
    "    # tokenize targets, can also sat padding as 'longest' to save memory and pad only to the longest target in the batch\n",
    "    labels = TOKENIZER(targets, max_length=1024, truncation=True, padding=False)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Tokenize the validation dataset\n",
    "tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea190bd9",
   "metadata": {},
   "source": [
    "# 5_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0302ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for Seq2Seq models used for padding and creating attention masks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=TOKENIZER, model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78a1989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # decode predictions and labels\n",
    "    decoded_preds = TOKENIZER.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, TOKENIZER.pad_token_id)\n",
    "    decoded_labels = TOKENIZER.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU expects list of references for each prediction (hence [[ref1], [ref2], ...])\n",
    "    decoded_labels = [[label.split()] for label in decoded_labels]\n",
    "    decoded_preds = [pred.split() for pred in decoded_preds]\n",
    "    \n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"bleu\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f521c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,  # important for seq2seq tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f749143",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=MODEL,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    processing_class=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b32b15",
   "metadata": {},
   "source": [
    "# 6_Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "ashtra_mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
